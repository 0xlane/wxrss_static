<?xml version="1.0" encoding="utf-8"?><rss version="2.0"><channel><title>Hugging Face</title><link>https://wxrss.reinject.top/94cb3d601b79c2c8db86eff4a83a07f9/</link><description>An RSS feed.</description><language>zh-cn</language><lastBuildDate>Sun, 26 Oct 2025 03:32:53 +0800</lastBuildDate><generator>wxrss -- https://github.com/0xlane/wxrss</generator><item><title>社区供稿丨RoboChallenge全球首发：重塑具身智能基准测试，开启真机评测新纪元</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495462&amp;idx=1&amp;sn=54ced239a7e30f227a7c30c025e22f10&amp;chksm=c33017d026f0a62739035d48ddf9788e9505ef92b784f199077070a096dd6c2dcac235a71994&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[近日，全球首个大规模、多任务的在真实物理环境中由真实机器人执行操作任务的基准测试——RoboChallenge重磅推出；通过科学的评估体系构建一个开放、公正、可复现的「真实考场」，克服真实环境下的性能]]></description><author>Hugging Face</author><pubDate>Thu, 16 Oct 2025 18:00:00 +0800</pubDate></item><item><title>开源点燃具身智能——前行者的炉边对谈</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495452&amp;idx=1&amp;sn=146dcfc2896dbfa70faf58460fae1dd6&amp;chksm=c31a979f018b8b4bcc4e8de65fcff4e13f36cf3cf0882c64c77b90ce89f7af47de49a51c5bf9&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[]]></description><author>Hugging Face</author><pubDate>Wed, 15 Oct 2025 22:10:00 +0800</pubDate></item><item><title>中秋快乐🎑</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495346&amp;idx=1&amp;sn=828d678b708166857e2bbf64bc03c165&amp;chksm=c32ab1bf9ffec3e6443f1d68d5f6b14af73990cd630fd90179e5b22491da3c465ad67fc61ef2&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[]]></description><author>Hugging Face</author><pubDate>Mon, 06 Oct 2025 10:00:00 +0800</pubDate></item><item><title>社区供稿丨MiniCPM-V 4.5 技术报告正式出炉</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495323&amp;idx=1&amp;sn=4c956461f403141199f29a91195683a1&amp;chksm=c34c44b5b7d4a09aeb0a585701dd8bee5f4cbe0dd1ca2b2b31e327b3beff159fb358b2f9627d&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[上个月，面壁小钢炮带来了最新的多模态模型 MiniCPM-V 4.5，成为行业首个具备“高刷”视频理解能力的多模态模型。模型一经开源广受社区好评，直接登上 HuggingFace Trending T]]></description><author>Hugging Face</author><pubDate>Wed, 24 Sep 2025 22:45:00 +0800</pubDate></item><item><title>Gaia2 与 ARE：赋能社区的智能体评测</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495318&amp;idx=1&amp;sn=b60e494c7777facdd5531368bebb44c4&amp;chksm=c3c07225f4c39b7c2893242c4db3d45cf35359504ff0718585bbfe988a9342ad1a11dc3ddeac&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[在理想情况下，AI 智能体应当是可靠的助手。当接收到任务时，它们能够轻松处理指令中的歧义，构建逐步执行的计划，正确识别所需资源，按计划执行而不被干扰，并在突发事件中灵活适应，同时保持准确性，避免幻觉。]]></description><author>Hugging Face</author><pubDate>Tue, 23 Sep 2025 22:05:00 +0800</pubDate></item><item><title>社区供稿丨基座上新：MiniCPM 4.1 将「高效深思考」引入端侧</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495252&amp;idx=1&amp;sn=aa820f157dda0edb64aa5810e6abdc2f&amp;chksm=c3d5c4092889ececba981856dd87df5c9a009ecc5345bbb764135d7a8c02d6e98aa138277532&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[💡  MiniCPM 4.1 亮点一览 🏃首个原生稀疏架构的深思考模型，通过可训练稀疏注意力创新，代码、数学推理等任务的推理速度比同尺寸开源模型快 3 倍以上；🌟在知识、推理、编程、指令遵循等综合能]]></description><author>Hugging Face</author><pubDate>Fri, 12 Sep 2025 18:00:00 +0800</pubDate></item><item><title>社区供稿丨揭秘端到端文档OCR模型 POINTS-Reader</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495247&amp;idx=1&amp;sn=e785db53781bfbb8a825f1623c19c906&amp;chksm=c31e735ec369c0f56b33146611618ca7c16de926ff36f1ed7e911bcc35808944d9ddd00201a7&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[目前论文已经被 EMNLP 2025 主会录取，同时模型已经开源论文: https://huggingface.co/papers/2509.01215Github: https://github.c]]></description><author>Hugging Face</author><pubDate>Thu, 11 Sep 2025 18:00:00 +0800</pubDate></item><item><title>ZeroGPU Spaces 加速实践：PyTorch 提前编译全解析</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495245&amp;idx=1&amp;sn=198511d331b8e51e79fcf1ba57f97fd0&amp;chksm=c38841f731c9e91696c00b71568f5238058b732d678db04635feee3e7bcff4d0a1ee81aa9775&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[ZeroGPU 让任何人都能在 Hugging Face Spaces 中使用强大的 Nvidia H200 硬件，而不需要因为空闲流量而长期占用 GPU。  它高效、灵活，非常适合演示，不过需要注意]]></description><author>Hugging Face</author><pubDate>Thu, 04 Sep 2025 21:00:00 +0800</pubDate></item><item><title>社区供稿 | 具身智能家务机器人黑客松报名开启！</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495162&amp;idx=1&amp;sn=50696ed77d5587f04b00217aec40cc92&amp;chksm=c35a510bf00e65b8bf8545d31253c6a66e90d1b56323125763c9ca3389a51cc8f8ef93724cfb&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[【黑客松双城启幕】全球首创家务机器人开发者黑客松，由 Hugging Face、NVIDIA 与 Seeed Studio 联合举办！我们将在深圳以及美国湾区举办两场开发者黑客松，中美双城联动。两大赛]]></description><author>Hugging Face</author><pubDate>Wed, 03 Sep 2025 18:00:00 +0800</pubDate></item><item><title>社区供稿 | 开源SOTA：阶跃发布端到端语音大模型Step-Audio 2 mini！</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495160&amp;idx=1&amp;sn=c467535edb75e091a37f57e5e33c7628&amp;chksm=c30065eb475e5efbe06b42663def5ad8cf44c84540c8deda08fac291a53732aa9aa0786fb768&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，今天阶跃星辰正式发布最强开源端到端语音大模型 Step-Audio 2 mini，该模型在多个国际基准测试集上取得 SOTA 成绩。它将语音理解、音频推理与生成统一建模，在音频理解、语音识别、]]></description><author>Hugging Face</author><pubDate>Tue, 02 Sep 2025 18:00:00 +0800</pubDate></item><item><title>社区供稿 | 开源多模态大模型新突破，书生·万象3.5发布，通用能力、推理能力与部署效率全面升级</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495153&amp;idx=1&amp;sn=90c7c6013625ffd3699bf7ac3cc6b68c&amp;chksm=c30ba261a329f1413adb222d723922b34c450f21bbcd1ec22407daead92189e4298c7066b3de&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[通往通用人工智能（AGI）的探索，不仅是技术的突破，更关乎体验的革新。多模态大模型通过融合视觉与语言等多维信息，让AI能够像人一样理解世界、解决问题，为用户带来更流畅、更可信赖的交互体验。在多模态大模]]></description><author>Hugging Face</author><pubDate>Mon, 01 Sep 2025 18:00:00 +0800</pubDate></item><item><title>社区供稿 | VibeVoice实现90分钟、多角色播客生成，拓展语音合成新边界</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495148&amp;idx=1&amp;sn=17cb0e38021aca4603e7f97763d47bfc&amp;chksm=c320fd7e9c5c59f55e55634bc31db3932492c4ac5cd919f084d0c868a4d1a7e08e18b45b400f&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[编者按：AI 自动生成播客早已不是新鲜事，但常见的 AI 播客只局限于几分钟的双人对话，这是因为传统语音生成模型大多基于离散化方法，更擅长生成短句、单一音色、结构规整的语音内容。近日，微软亚洲研究院提]]></description><author>Hugging Face</author><pubDate>Thu, 28 Aug 2025 18:00:00 +0800</pubDate></item><item><title>社区供稿 | 多模态新旗舰MiniCPM-V 4.5：8B 性能超越 72B，高刷视频理解又准又快</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495137&amp;idx=1&amp;sn=af897d12c18015eaeea4e60767893ab8&amp;chksm=c3b4cef03050ed3d858712ce1f7bae916eee7c849c95c90e74cc23291d02616b30e7594ef332&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[今天，我们正式开源 8B 参数的面壁小钢炮 MiniCPM-V 4.5 多模态旗舰模型，成为行业首个具备“高刷”视频理解能力的多模态模型，看得准、看得快，看得长！高刷视频理解、长视频理解、OCR、文档]]></description><author>Hugging Face</author><pubDate>Wed, 27 Aug 2025 17:00:00 +0800</pubDate></item><item><title>社区供稿 | 阿里国际Ovis2.5重磅发布：以小博大，刷新开源模型性能新高度</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495132&amp;idx=1&amp;sn=e287fad5658dc806d17b0c51cbf1b721&amp;chksm=c34fbece621cc8d2fc099da528a6ef7b0e41728d8799ff0718f688a996bf84241e9676d61ff8&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[阿里国际正式发布新一代多模态大模型Ovis2.5。Ovis2.5 是一款面向原生分辨率视觉感知、深度推理与高性价比场景设计的多模态大模型。在主流多模态评测套件 OpenCompass 上的综合得分相较]]></description><author>Hugging Face</author><pubDate>Tue, 26 Aug 2025 21:30:00 +0800</pubDate></item><item><title>直播预告｜开源生态如何引领具身智能的未来？</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495127&amp;idx=1&amp;sn=8728b61f37380c49ab7f00601e97a262&amp;chksm=c3979f3afc006d89b9c1a2ceac3c2c50fc2574bceb3cb73ab2920ebc31a5a6fb7c93fdb0775a&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[]]></description><author>Hugging Face</author><pubDate>Mon, 25 Aug 2025 22:00:00 +0800</pubDate></item><item><title>HF Papers 直播｜ 多模态专场</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495102&amp;idx=1&amp;sn=1b43c92f3e03ded888811ac410d13da1&amp;chksm=c3737736af76f759205673bdf4d3531bea1f4fce812f0a46a120b026af2201eb76be9b3efdbd&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[由 Hugging Face × OpenMMLab × ModelScope × 知乎 × 机智流 等联合发起的【AI Insight Talk】系列直播活动第四场 - 多模态专场，即将开播！近期各]]></description><author>Hugging Face</author><pubDate>Wed, 20 Aug 2025 21:30:00 +0800</pubDate></item><item><title>开源开发者须知：欧盟《人工智能法案》对通用人工智能模型的最新要求</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495092&amp;idx=1&amp;sn=5e5c200f1387977362a360e4abc897b3&amp;chksm=c3685729d52c5cb8f277d04966c809e059e919d3deed9b78d1f23e96e2cb7bdf28f9c60ce90b&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[⚠️ 本文不构成任何法律意见或建议。快速摘要 (TL;DR): 自 2025 年 8 月 2 日起，欧盟《人工智能法》将对通用人工智能（GPAI）模型的提供者模型提供者提出新的合规要求。但是对于用于科]]></description><author>Hugging Face</author><pubDate>Tue, 19 Aug 2025 19:00:00 +0800</pubDate></item><item><title>社区供稿 | MiniCPM-V4.0开源，多模态能力进化，手机可用，还有最全CookBook！</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495062&amp;idx=1&amp;sn=a82788e4a619d7e6b468993ac09dbb7b&amp;chksm=c3486f87b42066f4fa6031428402b34357aad866a0ca7f35c36a5045f792922c415e85842dbd&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[面壁小钢炮新一代多模态模型 MiniCPM-V 4.0 现已开源。依靠 4B 参数，取得 在 OpenCompass、OCRBench、MathVista 等多个榜单上取得了同级 SOTA 成绩，且]]></description><author>Hugging Face</author><pubDate>Fri, 08 Aug 2025 19:03:32 +0800</pubDate></item><item><title>欢迎 GPT OSS —— 来自 OpenAI 的全新开放模型家族！</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495056&amp;idx=1&amp;sn=7d8817150535304378dd35320b3777e9&amp;chksm=c389a134e97e5a4e791eaf56dbc33141094441d3b82608f5b2e602dd3ddd96bdcb1daaafc634&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[GPT OSS 是 OpenAI 推出的 重量级开放模型，面向强推理、智能体任务以及多样化开发场景。该系列包含两款模型：拥有 117B 参数的gpt‑oss‑120b和拥有 21B 参数的gpt‑os]]></description><author>Hugging Face</author><pubDate>Wed, 06 Aug 2025 16:30:00 +0800</pubDate></item><item><title>社区供稿 | GLM-4.5技术博客：原生融合推理、编码和智能体能力</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247495018&amp;idx=1&amp;sn=ea3077d86e8e14b107304f78a43445d6&amp;chksm=c3ab4f2ff66726425d4ae45e927c5a6113116cd6dc53ea3207bf0f860ee03012d9eff0aa668a&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[我们正式介绍两个新的 GLM 系列成员：GLM-4.5 和 GLM-4.5-Air——我们最新的旗舰模型。GLM-4.5 拥有 3550 亿总参数和 320 亿激活参数，而 GLM-4.5-Air 拥]]></description><author>Hugging Face</author><pubDate>Tue, 29 Jul 2025 21:00:00 +0800</pubDate></item><item><title>社区供稿 | 全能高手&amp;科学明星，上海AI实验室开源发布『书生』科学多模态大模型 Intern-S1 | WAIC 2025</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494994&amp;idx=1&amp;sn=3a599240e5db72d49d8f682c8a8e7a1a&amp;chksm=c391c3ca26bec64d4e897c3c4f36cb7c23518b539b23f02c017aaeae67567672462eeb0a7a3d&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[7 月 26 日，2025 世界人工智能大会（WAIC 2025）正式开幕。在当天下午举行的科学前沿全体会议上，上海人工智能实验室（上海AI实验室）发布并开源『书生』科学多模态大模型 Intern-S]]></description><author>Hugging Face</author><pubDate>Sat, 26 Jul 2025 20:31:01 +0800</pubDate></item><item><title>Hugging Face 开源机器人 Reachy Mini 开启预定</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494971&amp;idx=1&amp;sn=c3f702809b6daebd2cee74fa71f18b15&amp;chksm=c3a2e8c84714225690cc9da1dc0c9217d5d54f05860b9835bc71ea01f86fa7990e8bb306c7da&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[我们最新的开源机器人 Reachy Mini 正式亮相 🎉 这款富有表现力的开源机器人由 Pollen Robotics 与 Hugging Face 联合打造，专为人机交互、创意编程和 AI 实验而]]></description><author>Hugging Face</author><pubDate>Wed, 09 Jul 2025 22:00:00 +0800</pubDate></item><item><title>社区供稿 | Jina Embeddings V4: 为搜索而生，多模态多语言向量模型</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494933&amp;idx=1&amp;sn=28bf10d25b3a4e9ab2d1aed3dbe57fbe&amp;chksm=c36d0006cac80d30abcf145c203fae4cfec8af48fbc39c1253ae129b566971eff5b38ba10227&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[今天，我们正式发布 jina-embeddings-v4，一款全新的多模态向量模型，参数规模达到 38 亿，并首次实现了对文本与图像的同步处理。为了在各类检索任务中发挥极致性能，我们在模型内置了一套面]]></description><author>Hugging Face</author><pubDate>Fri, 27 Jun 2025 21:30:00 +0800</pubDate></item><item><title>nanoVLM: 简洁、轻量的纯 PyTorch 视觉-语言模型训练代码库</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494931&amp;idx=1&amp;sn=062426cb09fae41c039d487de90f1e33&amp;chksm=c3518d3362bba9ef23af0b3ee7075f8e180c6fc3c1387cdda6b978f461d43fe8baeb39c45ee9&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[nanoVLM是使用纯 PyTorch 训练 你自己的视觉语言模型 (VLM) 的 最简单 方式。它是一个轻量级 工具包 ，让你可以在免费的 Colab Notebook上启动 VLM 训练。nano]]></description><author>Hugging Face</author><pubDate>Wed, 18 Jun 2025 10:30:00 +0800</pubDate></item><item><title>HF Papers 直播｜ AI Insight Talk 强化学习专场</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494912&amp;idx=1&amp;sn=35d9b4baf8ca4fb6a6048b1e50611c7a&amp;chksm=c3344df1dc7ad71361445e3fbe6b28d87f1db92c6740595614c2ad4ea939e0421c4d12cc4043&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[由 Hugging Face × OpenMMLab × ModelScope × 知乎 × 机智流 联合发起的【AI Insight Talk】系列活动重磅来袭！本期我们将聚焦 强化学习（RL）与推]]></description><author>Hugging Face</author><pubDate>Thu, 12 Jun 2025 22:00:00 +0800</pubDate></item><item><title>SmolVLA: 让机器人更懂 “看听说做” 的轻量化解决方案</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494830&amp;idx=1&amp;sn=177343994164f129074f042c0b4c7fe1&amp;chksm=c34e6264dfe88501c21741c0f838041893a72b9def09767ea3e577cf8b548976185e4db4c8da&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[🧭 TL;DR今天，我们希望向大家介绍一个新的模型: SmolVLA，这是一个轻量级 (450M 参数) 的开源视觉 - 语言 - 动作 (VLA) 模型，专为机器人领域设计，并且可以在消费级硬件上运]]></description><author>Hugging Face</author><pubDate>Thu, 05 Jun 2025 10:30:00 +0800</pubDate></item><item><title>参加 Hugging Face 组织的 Gradio &amp; MCP 智能体主题黑客松</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494722&amp;idx=1&amp;sn=fc18dd47fcaf91328f415e43eaeef32f&amp;chksm=c36a21052334829158c776ce70e57ee72609b1cb4c80a1e6735d78e84f5ae9e78b6325e22ff7&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[🌟 欢迎参加 Gradio & MCP 智能体主题黑客松！准备好了吗？一场以智能体 (Agent) 和模型上下文协议 (Model Context Protocol，简称 MCP) 为核心的全球在线黑]]></description><author>Hugging Face</author><pubDate>Tue, 27 May 2025 23:00:30 +0800</pubDate></item><item><title>社区供稿 | Index-AniSora 技术升级开源: 动漫视频生成强化学习</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494708&amp;idx=1&amp;sn=c4c49d2ce70de004d4ec027ea9340f0f&amp;chksm=c37f1903cbb3ea7a38c01552c0b7b424c18176122b6a8af84da547d008ed82fb2833deb6a34a&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[B 站升级动画视频生成模型 Index-AniSora 技术并开源，支持番剧、国创、漫改动画、VTuber、动画 PV、鬼畜动画等多种二次元风格视频镜头一键生成！整个工作技术原理基于 B 站提出的 A]]></description><author>Hugging Face</author><pubDate>Wed, 21 May 2025 10:30:00 +0800</pubDate></item><item><title>大模型评估排障指南 | 关于可复现性</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494703&amp;idx=1&amp;sn=d25d001df8ec952cf604b75f9afea96d&amp;chksm=c3377be3544be69fa4c6116800fae12269b1c73c528ede016783928853f95e70690fdb6b3021&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[这是 大模型评估排障指南 系列文章的第三篇，敬请关注系列文章:关于推理关于  公式解析关于可复现性假设你读了一篇最近的新模型技术报告，然后心血来潮想要在本机复现他们的结果，却发现根本没法复现，这是为什]]></description><author>Hugging Face</author><pubDate>Tue, 13 May 2025 10:30:00 +0800</pubDate></item><item><title>社区供稿 | 智源研究院发布开源中文互联网语料库 CCI 4.0 新增高质量英文数据与合成数据</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494687&amp;idx=1&amp;sn=14731860d37909e721a7c20ab3eee50a&amp;chksm=c3df9c68100b6d73ff80dbb8f7988e398ddc8f2ba0c6bd1cab7848d1d22b09a90d9db2aa6488&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[2025 年 5 月 6 日，智源研究院在法国巴黎举行的 GOSIM 全球开源创新论坛上发布大型开源文本数据集 CCI 4.0，为全球的大模型创新发展再次提供重要的开源资源，并积极推动全球开源合作。C]]></description><author>Hugging Face</author><pubDate>Fri, 09 May 2025 10:30:00 +0800</pubDate></item><item><title>大模型评估排障指南 | 关于 LaTeX 公式解析</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494682&amp;idx=1&amp;sn=54a15bb466cf19990765f4d4c65057c2&amp;chksm=c33936a0193ad317f88c3eb7d2c9ec5e1fc96060f9840a1f83b2ca4e4674475e339ffa0cd7aa&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[这是 大模型评估排障指南 系列文章的第二篇，敬请关注系列文章:关于推理关于  公式解析关于可复现性解析 LaTeX 很难。这个问题在评估输出为  的模型时经常会遇到，例如 Hugging Face 的]]></description><author>Hugging Face</author><pubDate>Wed, 07 May 2025 22:00:00 +0800</pubDate></item><item><title>设计即合规: 开放AI生态中的用户数据治理实践</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494668&amp;idx=1&amp;sn=b41ca7c7a9c53bb79abe45dbe6f8650d&amp;chksm=c369e1bf43d1642304852114e0f9251ad7b5fabe258d2382ea9545ae115416ddd3da987de060&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[Hugging Face Hub 已成为 AI 协作的核心平台，托管了数万个模型、数据集以及交互式应用程序 (Hugging Face Space)。 在开放生态系统中，用户知情同意的管理方式与那些更]]></description><author>Hugging Face</author><pubDate>Wed, 30 Apr 2025 22:00:00 +0800</pubDate></item><item><title>社区供稿 | 阶跃星辰开源图像编辑模型 Step1X-Edit: 人人都能用的“改图大师”！</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494640&amp;idx=1&amp;sn=ce651e68903651ebd1fedc5156d25845&amp;chksm=c3c2578159f325b58904e789f431d327589b378b5e33459aba946db74338fd2cb73b89c98cec&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[阶跃星辰正式发布并开源图像编辑大模型 Step1X-Edit，性能达到开源 SOTA。该模型总参数量为 19B (7B MLLM + 12B DiT)，具备语义精准解析、身份一致性保持、高精度区域级控]]></description><author>Hugging Face</author><pubDate>Tue, 29 Apr 2025 22:00:00 +0800</pubDate></item><item><title>大模型评估排障指南 | 关于推理</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494635&amp;idx=1&amp;sn=2ab54e03ce453f5fbe8e8fd0ebbb9db5&amp;chksm=c3de8e0cfefb32eb1754d8f9190a5a0dcc4c76b0db19bbe774ecba904e3ee75093738db0708d&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[这是 大模型评估排障指南 系列文章的第一篇，敬请关注系列文章:关于推理关于  公式解析关于可复现性模型运行非常慢怎么办？调整 batch size如果你想要评估结果完全可复现 (在特定的输入 prom]]></description><author>Hugging Face</author><pubDate>Thu, 24 Apr 2025 23:00:00 +0800</pubDate></item><item><title>社区供稿 | 书生·万象 3.0 升级发布，创新预训练方法</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494624&amp;idx=1&amp;sn=5444e6caf442a5751ed9a1ddfd747f30&amp;chksm=c327f8c2c8d0fd5b16f12c7331a3cfb918c36ea892fa4aeb2b2b888736fd34846f01280d3b4f&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[驱动科学研究的人工智能正逐渐改变科学研究的模式，在探索以通专融合实现通用人工智能 (AGI) 的进程中，通用基座大模型，尤其是具备跨模态理解能力的多模态大模型至关重要——多模态大模型的创新突破，将大幅]]></description><author>Hugging Face</author><pubDate>Fri, 18 Apr 2025 21:30:00 +0800</pubDate></item><item><title>社区供稿 | 3700 次预训练总结超参规律，开源海量实验，告别盲猜</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494619&amp;idx=1&amp;sn=3c679a192800fd51c2ce81da4c17661a&amp;chksm=c3554f2349b610de9625c023ae4e5c137ccbc39d9a77553a67963ba14fb66df0e0cad17452db&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[近年来，大语言模型 LLMs 在多种任务上的卓越表现已得到广泛认可。然而，要实现其高效部署，精细的超参数优化至关重要。为了探究最佳超参数的规律，我们开展了大规模的实证研究，通过在不同配置上进行网格搜索]]></description><author>Hugging Face</author><pubDate>Thu, 17 Apr 2025 10:30:00 +0800</pubDate></item><item><title>社区供稿 | jina-reranker-m0 全新多模态多语言重排器</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494614&amp;idx=1&amp;sn=ec940792ea43e15429e7bd7ba67cb547&amp;chksm=c3847777948be6a47f2aa317a7bb290abfa13e43759ee0c2aa7d33f3b922e583991b5b910c4d&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[今天，我们正式发布jina-reranker-m0。这是一款多模态、多语言重排器 (reranker)，其核心能力在于 对包含丰富视觉元素的文档进行重排和精排，同时兼容跨语言场景。当用户输入一个查询]]></description><author>Hugging Face</author><pubDate>Thu, 10 Apr 2025 10:30:00 +0800</pubDate></item><item><title>让 LLM 来评判 | 技巧与提示</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494609&amp;idx=1&amp;sn=28cff6626cd93577ffcaba62a6376992&amp;chksm=c3a795ae64289dc673e4e12d9198671ad36962d18753314fb3563aac38c4b50873baabdb5ade&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[这是 让 LLM 来评判 系列文章的第六篇，敬请关注系列文章:基础概念选择 LLM 评估模型设计你自己的评估 prompt评估你的评估结果奖励模型相关内容技巧与提示LLM 评估模型已知偏差及缓解措施缺]]></description><author>Hugging Face</author><pubDate>Wed, 09 Apr 2025 10:30:00 +0800</pubDate></item><item><title>Open R1 项目进展第三期</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494605&amp;idx=1&amp;sn=550294c356734a34597ce6b647c0b1ff&amp;chksm=c3340b2876f680d8e27f3524348f070d85af7fdd4c4182bcb17b457e302fcc2da64b06e34129&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[原文发布于 2025 年 3 月 11 日本次更新带来三大突破性进展:CodeForces-CoTs 数据集: 通过 R1 模型蒸馏生成近 10 万条高质量编程思维链样本，同时包含 C++ 和 Pyt]]></description><author>Hugging Face</author><pubDate>Thu, 03 Apr 2025 10:30:00 +0800</pubDate></item><item><title>Open R1 项目进展第二期</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494570&amp;idx=1&amp;sn=aa4b15954068d3e223c026769847a5cd&amp;chksm=c32745a8e5143ba8718458afd28a4fd1460495626000f0d054518c5ec3b22d55f2cee46c4bff&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[原文发布于 2025 年 2 月 10 日我们启动 Open R1 项目已经两周了，这个项目是为了把 DeepSeek R1 缺失的部分补齐，特别是训练流程和合成数据。https://github.c]]></description><author>Hugging Face</author><pubDate>Mon, 31 Mar 2025 20:30:00 +0800</pubDate></item><item><title>Open R1 项目进展第一期</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494568&amp;idx=1&amp;sn=5bdd4bd7bd743586c2b23017c80c70a7&amp;chksm=c35a9d2055ee04f27f134c278592db760289434f6aaffac6eb02fec2588db8655aeb1abaf393&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[DeepSeek R1 发布已经两周了，而我们启动 open-r1 项目——试图补齐它缺失的训练流程和合成数据——也才过了一周。这篇文章简单聊聊:https://github.com/huggingf]]></description><author>Hugging Face</author><pubDate>Fri, 28 Mar 2025 10:30:00 +0800</pubDate></item><item><title>为什么 AI 模型离科学革命还差得很远？</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494506&amp;idx=1&amp;sn=36567330dd1b9e65ceab218b98d26bea&amp;chksm=c34a2216b1bc0954b23b43132e1180acf9ee8e710627313957d026bf8dd06f2caf720c55916e&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[作者：Thomas Wolf, Hugging Face 联合创始人和首席科学家发布日期：2025 年 2 月 26 日原文链接：🔭 The Einstein AI modelhttps://thom]]></description><author>Hugging Face</author><pubDate>Wed, 26 Mar 2025 23:31:16 +0800</pubDate></item><item><title>常见的 AI 模型格式</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494492&amp;idx=1&amp;sn=49997c031857a0a8a6f68cef1d976eb3&amp;chksm=c33e06b09d720a02fd9d26d0c2930830e463839707c204f98feeadb697b027e93dbaa02158f8&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[来源:https://blog.ngxson.com/common-ai-model-formats过去两年，开源 AI 社区一直在热烈讨论新 AI 模型的开发。每天都有越来越多的模型在Hugging]]></description><author>Hugging Face</author><pubDate>Tue, 25 Mar 2025 20:30:00 +0800</pubDate></item><item><title>SmolVLM2: 让视频理解能力触手可及</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494471&amp;idx=1&amp;sn=379a48aafa6d99b846219e69f9dced3b&amp;chksm=c301d621f96bb933e1e8ce3e07327e488ab5517837f0dd5375a86957703b785a36064565caef&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[一句话总结: SmolVLM 现已具备更强的视觉理解能力📺SmolVLM2 标志着视频理解技术的根本性转变——从依赖海量计算资源的巨型模型，转向可在任何设备运行的轻量级模型。我们的目标很简单: 让视频]]></description><author>Hugging Face</author><pubDate>Fri, 21 Mar 2025 23:00:00 +0800</pubDate></item><item><title>社区供稿 | 阶跃星辰 Step-Video-TI2V 图生视频模型开源，运动可控，动漫效果尤佳！</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494470&amp;idx=1&amp;sn=162bf4477f95d8db07f4b939e57ea174&amp;chksm=c39f8191128a7a6f6ce323518694190b759547d69771d8b74d683d62c9739055cd60c0aaf601&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[在今年 2 月，阶跃星辰开源了两款 Step 系列多模态大模型——Step-Video-T2V 视频生成模型和 Step-Audio 语音模型，为开源社区贡献了自己的多模态力量。今天，我们再接再厉，继]]></description><author>Hugging Face</author><pubDate>Thu, 20 Mar 2025 19:00:00 +0800</pubDate></item><item><title>在 Hugging Face Spaces 上使用 Gradio 免费运行 ComfyUI 工作流</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494467&amp;idx=1&amp;sn=35b85f858932c1e30d9d89c518cab726&amp;chksm=c3744d915574a85ce9e4a4776ded3a0ace2fdf9da7e7f9de014181c072d918ef42bb040c8c95&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[简介在本教程中，我将逐步指导如何将一个复杂的 ComfyUI 工作流转换为一个简单的 Gradio 应用程序，并讲解如何将其部署在 Hugging Face Spaces 的 ZeroGPU 无服务器]]></description><author>Hugging Face</author><pubDate>Tue, 18 Mar 2025 22:30:00 +0800</pubDate></item><item><title>在 Hugging Face 上部署语音转语音模型</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494344&amp;idx=1&amp;sn=2619b79c72bcc42a4f9fc5ed4cc5515e&amp;chksm=c342c9f44fe68b8eeec50b56ca082cdc1f081abca876f0c6c64966626949f1968544e3b05806&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[介绍S2S (语音到语音)是 Hugging Face 社区内存在的一个令人兴奋的新项目，它结合了多种先进的模型，创造出几乎天衣无缝的体验: 你输入语音，系统会用合成的声音进行回复。https://g]]></description><author>Hugging Face</author><pubDate>Tue, 11 Mar 2025 10:30:00 +0800</pubDate></item><item><title>LayerSkip: 使用自推测解码加速大模型推理</title><link>http://mp.weixin.qq.com/s?__biz=Mzk0MDQyNTY4Mw==&amp;mid=2247494343&amp;idx=1&amp;sn=038e055cb11ba3712ef64e7f1f4d9e61&amp;chksm=c319a398ee33e1d165dbfe3c6f4ecf0782d334740b832b63293f7f5723bd28e1d078c8d3bdbe&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[自推测解码是一种新颖的文本生成方法，它结合了推测解码 (Speculative Decoding) 的优势和大语言模型 (LLM) 的提前退出 (Early Exit) 机制。该方法出自论文Layer]]></description><author>Hugging Face</author><pubDate>Mon, 10 Mar 2025 10:30:00 +0800</pubDate></item></channel></rss>