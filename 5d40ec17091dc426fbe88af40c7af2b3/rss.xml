<?xml version="1.0" encoding="utf-8"?><rss version="2.0"><channel><title>NLP PaperWeekly</title><link>https://wxrss.reinject.top/5d40ec17091dc426fbe88af40c7af2b3/</link><description>An RSS feed.</description><language>zh-cn</language><lastBuildDate>Sun, 01 Mar 2026 15:47:48 +0800</lastBuildDate><generator>wxrss -- https://github.com/0xlane/wxrss</generator><item><title>Qwen推出DeepPlanning Benchmark：Agent真正搞定购物和旅行规划了吗？</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247486177&amp;idx=1&amp;sn=91efe3d042930088ca8895579d5b8275&amp;chksm=cedb48c93f10c4ac86b0536c946666b434127779c664f6a61b53951d11993f4c82d3b5791652&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine，前段时间Qwen推出C端应用，跑通购物，外卖，旅行，支付等场景，但是Agent离真正跑通这些任务还差多远呢？Qwen团队推出DEEPPLANNING Benchmark进]]></description><author>NLP PaperWeekly</author><pubDate>Sat, 31 Jan 2026 11:55:17 +0800</pubDate></item><item><title>NIPS25最佳论文｜阿里Qwen提出「门控注意力」：极简改动，推开LLM效率与稳定新大门</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247486164&amp;idx=1&amp;sn=21e2a3709a937d593155b1d719177d97&amp;chksm=ce4a75fa5f4a51a71e301b0ddc1a13d8725cd056273eb25431e91b1afe00bd09a7d4f32e6e37&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine今天分享一篇Qwen在NIPS25的Best paper，标题为 "Gated Attention for Large Language Models: Non-linea]]></description><author>NLP PaperWeekly</author><pubDate>Wed, 28 Jan 2026 10:53:54 +0800</pubDate></item><item><title>从AlphaEvolve到ThetaEvolve：开源小模型如何通过“进化”掌握高深数学技巧？</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247486152&amp;idx=1&amp;sn=35db32c21e18090f42a63a5f4df410e5&amp;chksm=ce9989d1f5ba7e4b62a3d68c0c7c1e00397e5208236411f752982481161baf173462e24f81fa&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine，好久不见～今天分享一篇来自微软（Microsoft）和华盛顿大学（University of Washington）等机构合作的最新论文，Title: ThetaEvol]]></description><author>NLP PaperWeekly</author><pubDate>Mon, 22 Dec 2025 09:14:13 +0800</pubDate></item><item><title>深度解读：Alpha Arena背后细节，LLM距离“华尔街之狼”还有多远？</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247486118&amp;idx=1&amp;sn=56ca29c0ceae1db8f8aae389bd8ca6e4&amp;chksm=ce7626a3ff2206abf00e18fc97757599726edd39d3d82cabe8fa187503fb3c27f3bbab08afc7&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[今天分享一篇来自AI研究机构 Nof1 的文章，Title: Exploring the Limits of Large Language Models as Quant Traders（探索大型语]]></description><author>NLP PaperWeekly</author><pubDate>Wed, 05 Nov 2025 09:08:01 +0800</pubDate></item><item><title>Trade in Minutes! 揭秘首个将策略与执行完全分离的量化交易Agent</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247486098&amp;idx=1&amp;sn=4f1dff8cc9c861f7a84d1ef135696873&amp;chksm=ce006b646aeb97ab7e609bdd38020f5729ee52270b464d7a042efa313ab5c6903c3b1bbb6c2c&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[今天分享一篇来自同济大学与微软亚洲研究院等机构的文章，题为 《TRADE IN MINUTES! RATIONALITY-DRIVEN AGENTIC SYSTEM FOR QUANTITATIVE]]></description><author>NLP PaperWeekly</author><pubDate>Thu, 30 Oct 2025 17:05:47 +0800</pubDate></item><item><title>SOTA诞生！Meta发布AIRA-dojo框架，AI智能体Kaggle竞赛奖牌率提升至47.7%</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247486081&amp;idx=1&amp;sn=f5f72d315f82b63a4b584584b65c5747&amp;chksm=ce99de7f97cc797361d07374eedf44c0109d116241b70fa975b480171946c3844383cd599352&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine今天分享一篇来自Meta FAIR、伦敦大学学院（UCL）和厄勒布鲁大学的研究人员共同发表的文章，Title: Al Research Agents for Machine]]></description><author>NLP PaperWeekly</author><pubDate>Mon, 20 Oct 2025 17:15:28 +0800</pubDate></item><item><title>AI Agent落地必读：深度解读OpenAI 姚顺雨 的T-bench，如何评测智能体的“真功夫”</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247486049&amp;idx=1&amp;sn=1f26f0f9ac6fbec5e70976f35f61c035&amp;chksm=ce8c857a7083a0da1b7b66a163bba8e80ca0016d8fd509455c78d60706ee425d3066cf970a33&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine，今天继续来分享OpenAI 姚顺雨 的一篇文章，这篇文章探讨了Agent实际落地时该如何评测，他在the second half里面也提了这篇文章，是Agent真正落地到]]></description><author>NLP PaperWeekly</author><pubDate>Fri, 17 Oct 2025 11:34:16 +0800</pubDate></item><item><title>ACL 2025 | 蚂蚁GALLa：用图结构增强代码大模型，让代码理解更精准！</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247486039&amp;idx=1&amp;sn=a68d37cefa05afb5fd06cefcffae41c7&amp;chksm=ced3d671e647759d6b67f701a0854b394031fcb6d43a1e3b85756bb8ed43b1f230619f950041&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[▍简介本文源于蚂蚁集团与上海交通大学的校企合作项目，目前已被 ACL 2025 主会接收。ACL（Annual Meeting of the Association for Computational]]></description><author>NLP PaperWeekly</author><pubDate>Tue, 14 Oct 2025 09:47:31 +0800</pubDate></item><item><title>AgentFly：重塑Agent，无需微调LLM，如我们一样的记忆和经验持续学习</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247486037&amp;idx=1&amp;sn=f592da031421a748881e5cc986664fcd&amp;chksm=ce259a4f29dc8c99f93b6d07885006a07970e4e5c8f50ca4dc3312e376c12851199a56c3f81b&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[近年来，大型语言模型（LLM）智能体已成为AI领域的热点，它们能自主使用工具、进行多步推理，完成复杂任务，如深度研究、代码生成、多轮对话等。然而，现有的LLM智能体面临两大困境：一是依赖静态、手工设计]]></description><author>NLP PaperWeekly</author><pubDate>Sat, 11 Oct 2025 11:13:54 +0800</pubDate></item><item><title>当前AI-Scientist Benchmarks深度分析报告</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247486034&amp;idx=1&amp;sn=e173f027eece56e0d3a94ce545f5b6db&amp;chksm=ce88f430b3e1771c4cb951067415b91480f594f96e22ba1bdd9ee0df128be68685d3cb8865c6&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[1 概述1 TL;DR — 核心结论（要点）1. 目前衡量 “AI 当科研助理 / AI-Scientist” 的 benchmark 大致落在两条主线：（A）论文复现 / 代码实现（repro /]]></description><author>NLP PaperWeekly</author><pubDate>Fri, 19 Sep 2025 00:19:33 +0800</pubDate></item><item><title>再看SWE-Bench：论一个好的benchmark是如何推动2025 Agentic编程范式的发展</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247486029&amp;idx=1&amp;sn=35e77fb7f9b30596dcac6e272317c568&amp;chksm=cefb8fe4ec371c035a9afb350aa1f756d97e3e5ed375fcd166485a2aac965a86a41e238cbf03&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine～今天继续来看下经典的code benchmark之SWE-BENCH的细节,  其由普林斯顿大学和芝加哥大学联合发表于ICLR 2024，Title: SWE-bench]]></description><author>NLP PaperWeekly</author><pubDate>Thu, 18 Sep 2025 00:23:08 +0800</pubDate></item><item><title>清华大学｜从互联网架构视角，重新审视AI智能体通信的挑战与机遇</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247486027&amp;idx=1&amp;sn=00ab4026b0c04a06b9c651a42b08b38d&amp;chksm=ce8ff3c6a4f04c831c1a5f76c9dfbefe31dddb6a2b7f0350114a0da2b922168cb38eccc1fd43&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine。今天分享来自中关村实验室和清华大学的一篇观点文章：AI Agent Communication from Internet Architecture Perspectiv]]></description><author>NLP PaperWeekly</author><pubDate>Wed, 17 Sep 2025 09:31:10 +0800</pubDate></item><item><title>再看GAIA Benchamrk：他是如何推动Agent系统的发展的？</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485984&amp;idx=1&amp;sn=e6abe3a824ed5ba193053dc004f866ed&amp;chksm=ced686d888836df7c0c71665828941f6f8b36fa7213d154e6adfc0dbb95a76804ed684e686e4&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine。今天再来看看GAIA Benchmark的细节，来看看GAIA如何成为Agent领域最经典的BenchMark之一。它由Meta FAIR、Hugging Face、Au]]></description><author>NLP PaperWeekly</author><pubDate>Tue, 16 Sep 2025 00:05:04 +0800</pubDate></item><item><title>斯坦福最新研究：最强LLM也搞不定前沿代码！Gemini 2.5 Pro成功率不足40%</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485974&amp;idx=1&amp;sn=3aa03b788d856c53bf6bfb725e9218aa&amp;chksm=ce2ee10460c6b9e3a1e9f9eee21d200a9ac4ec67d34bfaf64ca4b7631a74dd55a537d7ab6096&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine今天分享一篇来自斯坦福大学的Benchmark文章，Title: ResearchCodeBench: Benchmarking LLMs on Implementing]]></description><author>NLP PaperWeekly</author><pubDate>Mon, 01 Sep 2025 11:23:53 +0800</pubDate></item><item><title>中国科大认知全重实验室发布 Science-Star（科星） : 一体化、可扩展的科学智能体搭建平台</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485961&amp;idx=1&amp;sn=ba2d967a1058c6a956e712469b2bb634&amp;chksm=ce1ae36856ea71de76f51ce3540c7ca84ac234f4dc6b319d0801fc1695007de1106d80d0ec68&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[中科大认知智能全国重点实验室开发实现 Science-Star 科研智能体平台。它基于 ReAct 引擎，融合了规划（Planning）、行动（Action）、记忆（Memory）、反思（Refle]]></description><author>NLP PaperWeekly</author><pubDate>Mon, 25 Aug 2025 14:28:09 +0800</pubDate></item><item><title>再看OpenAI PaperBench：Agent离独立复现AI论文还有多远？</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485955&amp;idx=1&amp;sn=e368a848a3eb871b3c49b4c38780c29d&amp;chksm=ce1b109fbf87aaea3fd1937898700605b08fa9c951ae34835f59e4e12674a8e40db5388710e0&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine今天分享一篇来自OpenAI的文章，Title: PaperBench: Evaluating AI's Ability to Replicate AI Research]]></description><author>NLP PaperWeekly</author><pubDate>Thu, 21 Aug 2025 09:43:14 +0800</pubDate></item><item><title>字节也来卷Agent工具调用了？FTRL: 无需外部工具，五步自动化构建训练环境，LLM工具调用能力飙升10%</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485914&amp;idx=1&amp;sn=4367bf35d097827aeab9c97a8eee2a2b&amp;chksm=ce06ac457b77ef87c2c8fb53958addfe136aee15a9b8cab543e54eacca0c6ae51ef81dde5e61&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine今天分享一篇来自字节跳动和复旦大学的研究，标题为 《通过自动化构建环境的反馈驱动方法提升大型语言模型的工具使用能力》 (Feedback-Driven Tool-Use I]]></description><author>NLP PaperWeekly</author><pubDate>Wed, 13 Aug 2025 17:36:10 +0800</pubDate></item><item><title>ICML 2025｜AI智能体也能自我进化？威斯康星大学提出MetaAgent框架，让AI自己设计AI</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485890&amp;idx=1&amp;sn=f474b2ab03827a88ed34a1f8137321e0&amp;chksm=ce83653b5fe142471b222ce041b0ba899221919d88a45d5820d223f4b7de6144e0c0a3e43a59&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine今天分享一篇来自威斯康星大学麦迪逊分校的文章，Title: MetaAgent: Automatically Constructing Multi-Agent System]]></description><author>NLP PaperWeekly</author><pubDate>Wed, 06 Aug 2025 16:54:05 +0800</pubDate></item><item><title>ICLR25 | 告别手搓workflow！AI 能自己设计更强智能体吗?</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485824&amp;idx=1&amp;sn=9adab5c75d0c249bebc88d4fc109e636&amp;chksm=ce0b4f4e8075e5a52b5d59908cb371a27efe9fe6f04ec798cc085013fc2e8795a468d9493f0a&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[今天分享一篇ICLR25的一篇文章，标题为：AUTOMATED DESIGN OF AGENTIC SYSTEMS（自动化Agent设计系统）。手写workflow太累啦，这篇文章探讨了如何自动化设]]></description><author>NLP PaperWeekly</author><pubDate>Fri, 11 Jul 2025 10:18:01 +0800</pubDate></item><item><title>CMU ｜LLM在数学推理能力的提升是否能迁移到其他领域？</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485811&amp;idx=1&amp;sn=27f40060aa2395d977ffec26000e6b5a&amp;chksm=cebf35f425c53541f8329ac093909fc9f4a4a0510210f2256b8a65440da13047cca97e25df90&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine今天分享一篇来自卡耐基梅隆大学、宾夕法尼亚大学、华盛顿大学、M-A-P 和香港理工大学的研究，标题为：《Does Math Reasoning Improve Genera]]></description><author>NLP PaperWeekly</author><pubDate>Sat, 05 Jul 2025 15:47:23 +0800</pubDate></item><item><title>DeepMind | 合成数据+RL，SWiRL让大模型拥有超强“工具脑”！</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485800&amp;idx=1&amp;sn=9df6919c519eec46a09e7757b539c27e&amp;chksm=cee2f631ac5826c726ddce8f013e0f2ad448b764cc0e9b6417ce35f5175c72e2c7398b632d17&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine今天分享一篇来自斯坦福大学和 Google DeepMind 的文章，标题为：《Synthetic Data Generation & Multi-Step RL for]]></description><author>NLP PaperWeekly</author><pubDate>Fri, 04 Jul 2025 15:55:30 +0800</pubDate></item><item><title>ACL25 | SURVEYFORGE: 让AI写出结构严谨、引用精准的高质量综述</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485766&amp;idx=1&amp;sn=23859acecc57cad1d1032006fe89e685&amp;chksm=cee052da4395209263fd9051b41a798426438e9e5bed96e9be1508f266daf81599ad4ca2a5a4&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine今天分享一篇来自上海人工智能实验室、复旦大学和上海交通大学 的研究论文，标题为《SURVEYFORGE: On the Outline Heuristics, Memory]]></description><author>NLP PaperWeekly</author><pubDate>Mon, 30 Jun 2025 23:21:42 +0800</pubDate></item><item><title>ACL25 | DOLPHIN，Closed-loop Auto-research系统来帮你自动做科研了！</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485738&amp;idx=1&amp;sn=758e32266e4bc2a0c2a3f1ecb8c04dd5&amp;chksm=ce76b41ec89a84cfd20531ce0786798e10e3e444b122ecfb4a1ef5f159eda0107c615aa731b1&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine今天分享一篇上海人工智能实验室 的文章，标题为 DOLPHIN: Moving Towards Closed-loop Auto-research through Thin]]></description><author>NLP PaperWeekly</author><pubDate>Tue, 10 Jun 2025 17:35:13 +0800</pubDate></item><item><title>成本暴降88%！通义实验室、北大发布ZeroSearch，无需搜索即可激活LLM检索能力</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485725&amp;idx=1&amp;sn=951dffa783dd6880539586fdd3a1a390&amp;chksm=cef59c47b6404825d81e725da6cc8db33f358076f3279f8bcf0ef19dcc1f980914531c38948d&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[本文作者来自通义实验室和北京大学，第一作者是北京大学智能学院博士生孙浩，主要研究方向是RAG和Agent，在 NeurIPS、ACL、EMNLP 等国际顶级会议上发表多篇论文，师从张岩教授。该工作在阿]]></description><author>NLP PaperWeekly</author><pubDate>Fri, 06 Jun 2025 09:40:54 +0800</pubDate></item><item><title>Harvard | RL如何放大pretraining过程中学到的行为？</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485719&amp;idx=1&amp;sn=4c209a6e8ad32d9cba75bf8639a3ea38&amp;chksm=ce02bf5a7e39cd13bcdf110cc9b6df323a371ffb8aae08080437f2a6f072fe8c1a7b20b732b7&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[今天分享一篇来自 Harvard University 和 Kempner Institute 的文章，标题为 Echo Chamber: RL Post-training Amplifies Be]]></description><author>NLP PaperWeekly</author><pubDate>Wed, 28 May 2025 13:01:58 +0800</pubDate></item><item><title>Microsoft | reward model需要用推理模型吗？</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485677&amp;idx=1&amp;sn=69c30b24c660215bc591b2310f719f70&amp;chksm=ce43df1cb6e16ed3d01f2c39e5d39096a013567b089736f064d5333576a8c7f4ce5b886501b0&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[今天分享一篇来自Microsoft公司、清华大学和北京大学的一篇文章，Title: Reward Reasoning Model: 奖励推理模型。这篇文章探索了一种名为“奖励推理模型（RRMs）”的]]></description><author>NLP PaperWeekly</author><pubDate>Fri, 23 May 2025 10:31:45 +0800</pubDate></item><item><title>去年错过黄金的人，现在都在疯抢它！</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485665&amp;idx=1&amp;sn=194bb838e9e928322acd8357e961cfed&amp;chksm=ceb827026341d1a6617be7524aaa6daaf754b1113e0a268d498fba679b380ade83cd424eeb3c&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[]]></description><author>NLP PaperWeekly</author><pubDate>Thu, 22 May 2025 07:22:43 +0800</pubDate></item><item><title>啥工作量？！60天就发了一篇NLP顶会</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485659&amp;idx=1&amp;sn=d08cb0def72705c995ca7449dcc01d71&amp;chksm=cec000297ced63c38bf28cbf2971051e771f0908f1b6343aeae51fe2400aad209ed826db2bde&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[搞科研，最怕的就是每天“眼睛读文献，脑袋想方案”。以为只要文献读的够多，准备就足够充分，就能找到好选题，写出好文章。实际上是在用“勤奋读文献”掩盖“不敢开始干”的焦虑。过来人都知道：科研成果是干出来的]]></description><author>NLP PaperWeekly</author><pubDate>Wed, 21 May 2025 09:10:00 +0800</pubDate></item><item><title>DeepMid | RL还需要value function吗?</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485652&amp;idx=1&amp;sn=96330f778b8af7b8d66c41b0d612db1e&amp;chksm=cefadeb0b6ef5c76853b17bd5a7633a2c4b08fcc93237430208e858f04be71e567d2c075f9d4&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[今天分享一篇来自 DeepMind 的研究论文，标题为：Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM]]></description><author>NLP PaperWeekly</author><pubDate>Mon, 19 May 2025 11:44:52 +0800</pubDate></item><item><title>Kaggle | 总奖池超200万美金的数学竞赛AIMO2冠军方案</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485629&amp;idx=1&amp;sn=4686ae7aad90c4bb53cdf4e03d6aa56e&amp;chksm=ce907f5663307cb5d1eb29b0dcc87548546a585b036531a30fdfd88af4dfcc4d9be8d67ca074&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[Kaggle | 总奖池超200万美金的数学竞赛AIMO2冠军方案大家好，我是HxShine今天分享一篇来自 NVIDIA 的文章（kaggle AIMO2冠军方案，冠军奖金高达26w美金，总奖池超]]></description><author>NLP PaperWeekly</author><pubDate>Mon, 28 Apr 2025 16:48:17 +0800</pubDate></item><item><title>无需标注数据！TTRL用“少数服从多数”解锁大模型测试时强化学习新范式</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485614&amp;idx=1&amp;sn=fce8341039018434dbb905f0d5e83e6a&amp;chksm=ceff59a6bd1e7a2106c98aa8d104f7ea895a491a9a53d940752e6015e5b07681a8de368a80c4&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine！今天分享一篇来自清华大学和上海人工智能实验室的文章，标题为 TTRL: Test-Time Reinforcement Learning（测试时强化学习）。这篇文章探讨了]]></description><author>NLP PaperWeekly</author><pubDate>Sun, 27 Apr 2025 14:28:53 +0800</pubDate></item><item><title>震惊！强化学习训练后，大模型推理“天花板”反而降低了？清华研究揭示RLVR局限性</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485603&amp;idx=1&amp;sn=bcd093568be440d9d74eceeb65d1b158&amp;chksm=ce262d672cfb61836a8acbb2943c66eaca722405c9bfc030d6e41a81f44e897b204733953de6&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine今天分享一篇来自清华的文章，标题为：“Does Reinforcement Learning Really Incentivize Reasoning Capacity i]]></description><author>NLP PaperWeekly</author><pubDate>Thu, 24 Apr 2025 15:23:18 +0800</pubDate></item><item><title>2025新风口！AI大模型又起飞了！</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485594&amp;idx=1&amp;sn=bc8c19ccf1190f979f7c2aa5cdd87ab6&amp;chksm=ce96df747beaeae266fb443fe082e39444e96d7baa4ce8e010543c1bfb6a2cfebd6223f0fe24&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[2025开年，AI技术打得火热，正在改变程序员的职业命运：阿里云核心业务全部接入Agent体系；字节跳动30%后端岗位要求大模型开发能力；腾讯、京东、百度开放招聘技术岗，80%与AI相关……大模型正在]]></description><author>NLP PaperWeekly</author><pubDate>Wed, 16 Apr 2025 09:24:28 +0800</pubDate></item><item><title>中国科大认知全重实验室发布Agent-R1训练框架，支持自主思考与工具调用！</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485585&amp;idx=1&amp;sn=0f54c262e996802d8dbf72f9fa657205&amp;chksm=ce60588df738772278ddd4b730e45fb3abcde56f2905abc28a06e9e060aadedef91c969338d7&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[什么是 Agent-R1imgAgent-R1是由中科大认知智能全国重点实验室开发的智能体强化学习训练框架，致力于推进强化学习与智能体技术的融合发展。框架采用端到端强化学习方法，突破了依赖人工设计工]]></description><author>NLP PaperWeekly</author><pubDate>Wed, 02 Apr 2025 09:19:38 +0800</pubDate></item><item><title>香港科技大学联合DeepSeek-AI推出CODEI/O：让AI学会“浓缩”推理模式</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485575&amp;idx=1&amp;sn=c064333803739395f375053af501900f&amp;chksm=ce8f3af7ebbc27301e8bc5664a05ff495d5ec18ab068259668f5b87b23d1961985a7a65cf685&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[大家好，我是HxShine今天分享一篇香港科技大学、DeepSeek-AI和上海交通大学联合发表的文章，标题为：CODEI/O: Condensing Reasoning Patterns via]]></description><author>NLP PaperWeekly</author><pubDate>Tue, 01 Apr 2025 07:28:57 +0800</pubDate></item><item><title>人工标注太贵，合成数据不够好？看OS-Genesis如何破解数据困局</title><link>http://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&amp;mid=2247485546&amp;idx=1&amp;sn=7baa1b27658e134744007872d97607bd&amp;chksm=ce2f83999bad77cf281ad0bdf6641490210647967f8873262a6e8a23672c1ed99d9d3a3450b2&amp;scene=0&amp;xtrack=1#rd</link><description><![CDATA[从1920年的小说《R.U.R》到《钢铁侠》中的JARVIS，在过去的一个世纪里，人们一直梦想着构建能够自动化日常工作的Digital Agents（数字代理）。如今，随着视觉语言模型（VLMs）的]]></description><author>NLP PaperWeekly</author><pubDate>Mon, 31 Mar 2025 10:55:08 +0800</pubDate></item></channel></rss>